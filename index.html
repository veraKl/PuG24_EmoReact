<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="Exploring Emotion Processing in the Human Brain through Positive and Negative Affect-inducing GIFs"/>
  <meta property="og:description" content="Poster for PuG 2024"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Exploring Emotion Processing in the Human Brain through Positive and Negative Affect-inducing GIFs</title>
  <link rel="icon" type="image/x-icon" href="static/images/brain-512.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">Exploring Emotion Processing in the Human Brain through Positive and Negative Affect-inducing GIFs</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.researchgate.net/profile/Joshua-Rocha-2" target="_blank">Joshua A. Rocha</a><sup>1,5</sup>,</span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/vera-klütz-5a67a1283" target="_blank">Vera Klütz</a><sup>1,6</sup>,</span>
                  <span class="author-block">
                    <a href="https://www.zi-mannheim.de/en/research/people/person/15954.html" target="_blank">Gordon B. Feld</a><sup>1,2,3,4</sup>,</span>
                    <span class="author-block">
                      <a href="https://skjerns.de" target="_blank">Simon Kern</a><sup>1,2,3,4</sup></span>
                  </div>
                  <div class="is-size-6 publication-authors">
                    <span class="author-block">1 Department of Clinical Psychology, Central Institute of Mental Health, Medical Faculty Mannheim, University of Heidelberg, Mannheim, Germany
                      <br>2 Department of Psychiatry and Psychotherapy, Central Institute of Mental Health, Medical Faculty Mannheim, University of Heidelberg, Mannheim, Germany
                      <br>3 Department of Addiction Behaviour and Addiction Medicine, Central Institute of Mental Health, Medical Faculty Mannheim, University of Heidelberg, Mannheim, Germany
                      <br>4 Institute of Psychology, Faculty of Behavioural and Cultural Studies, University of Heidelberg, Heidelberg, Germany
                      <br>5 Heidelberg Academy of Sciences and Humanities, Heidelberg, Germany
                      <br>6 Institute of Cognitive Science, Osnabrück University, Osnabrück, Germany</span>
                      <span></span><a href="https://pug2024.de/index-en.html" target="_blank">Psychology & Brain 2024</a></span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>-->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="#poster-section" class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Poster</span>
                      </a>
                    </span>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div style="display: flex; justify-content: center; align-items: center;">
        <img src="static/videos/posi_0605.gif" autoplay loop height="150%" muted>
      </div>
      <h2 class="subtitle has-text-centered">
        <br>We aim to establish a machine learning algorithm capable of classifying positive and negative affect markers in brain signals of humans
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Poster Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Emotions are thought to influence activity in human brain areas that control decisions, direct attention, and motivate behavior in our surrounding world. Indeed, affective neuroscience has long since attempted to explore the underlying mechanisms of emotion processing that are interlocked with perception, cognition, motivation, and action in the brain. However, the organization of the anatomical and functional neural networks that overall form our emotion processing architecture are yet to be fully elucidated.<br>
            This study aimed to investigate this by measuring the positive and negative affect of a set of short video clips (GIFs), which have previously been validated and classified by thousands of participants into 27 distinct emotion categories, using Magnetoencephalography (MEG). Healthy participants are placed in the MEG scanner and subjected to a behavioral psychophysics task, in which a total of 144 positive or negative affect-inducing GIFs are shown in a randomized order and subsequently rated on a 5-point scale of the valence and arousal dimensions in each trial. As a next step (currently in progress), the obtained MEG signals will be analyzed using machine learning algorithms to extract neural markers of positive and negative affect processing.<br>
            These preliminary findings could potentially lead to an improved understanding of the neural networks involved in emotion processing and furthermore facilitate the development of novel translational approaches against affective disorders such as major depression and bipolar disorder, along with methods to detect affect processing in the absence of behavioral input in cases such as sleep or resting state memory consolidation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 is-centered has-text-centered">Experiment</h2>
       <div style="display: flex; justify-content: center; align-items: center;">
        <img src="static/images/JR_ERP_ExpDes_FC.png" autoplay loop height="100%" muted>
      </div>
      <h2 class="subtitle has-text-centered">
      <br><br>
      <br><br>
    </h2>
     
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay loop height="100%" muted>
        <!-- Your video here -->
        <source src="static/videos/JR_PuG_ExpMeth.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <b>An example of an experiment round</b>
      </h2>
        <h2 class="subtitle has-text-justified">
        In each trial, a red fixation cross first appears at the center of one of four quadrants on the screen to draw the attention of the participant to that part of the screen. 
        <br>One of the 144 GIFs then plays out and the participant pushes a button on a controller to indicate the moment they felt an emotion. 
        <br>The participant subsequently rates how positive or negative the emotion they felt was on a 5-point Valence scale, and how calm or excited the emotion they felt was on a 5-point Arousal scale. 
        <br>Finally, the participant performs a flanker task in which four arrows appear at four possible locations (north, south, east, or west) in a randomized order and they press buttons on the controller to indicate the direction the arrow is pointed towards.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->


<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 is-centered has-text-centered">Data Analysis</h2>
     
    <br><br><br>
    <h3 class="title is-4">Preprocessing</h3>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/humandeep1.PNG" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          THe MEG data went through the following preprocessing steps:
          - Highpass-, lowpass- and notch-filtering
          - Event extraction and epoching
          - Autorejection of bad epochs
          - ICA with rejection of ECG and EOG related signals
          <br>
          Additionally, as recent literature suggests only minimal preprocessing, a second version has been obtained with only highpassed epochs. Despite a bigger confidence interval for minimal preprocessed data, no difference in classification accuracy could be found.
        </h2>
        </div>
        <div class="item">
        <!-- Your image here -->
        <img src="static/images/humandeep2.PNG" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Results showed significantly high correlations in response patterns between humans and models, suggesting similarities in classification behaviors. When analyzing error distributions by excluding correct responses, correlations between humans and neural networks were found to be low to moderate, with ResNet and VGG-16 showing the highest similarity to human errors. However, divergent classification patterns between humans and deep neural networks (DNNs) emerged when the level of image distortion increased, with misclassifications often occurring among conceptually similar classes, suggesting that current neural network architectures may need further refinement to better emulate human behavior.
        </h2>
      </div>
    </div>


  </div>
</section>
<!-- End youtube video -->


<!-- Video carousel -->
<section class="hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <br><br><br>
        <h2 class="title is-3 is-centered has-text-centered">Discussion</h2>
        <div class="content has-text-justified">
          <p>
            In this project, we aim to find a classifier that can successfully decode emotions. The preliminary findings could potentially lead to an improved understanding of the neural networks that are involved in emotion processing in humans. This would open up opportunities to facilitate the progress of novel translational approaches against increasingly relevant affective disorders such as major depression and bipolar disorder. Simultaneously, the development of methods that detect emotion affect processing in the absence of behavioral input in cases such as when one is asleep or when the memory consolidation process occurs during resting state.
          </p>
          <br>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->






<!-- Paper poster -->
<section class="hero is-small is-light" id="poster-section">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-centered has-text-centered">Poster</h2>

      <iframe  src="static/pdfs/JR_VK_PuG_Poster_ZI_hoch_final.pdf" width="100%" height="1400">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title is-centered has-text-centered">Literature</h2>
      <pre><code>
Brosch, T., Scherer, K., Grandjean, D., & Sander, D. (2013). The impact of emotion on perception, attention, memory, and decision-making. Swiss Medical Weekly, 143(1920), w13786. https://doi.org/10.4414/smw.2013.13786
Cowen, A. S., & Keltner, D. (2017). Self-report captures 27 distinct categories of emotion bridged by continuous gradients. Proceedings of the National Academy of Sciences, 114(38), E7900 – E7909. https://doi.org/10.1073/pnas.1702247114
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
