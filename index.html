<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="Exploring Emotion Processing in the Human Brain through Positive and Negative Affect-inducing GIFs"/>
  <meta property="og:description" content="Poster for PuG 2024"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Exploring Emotion Processing in the Human Brain through Positive and Negative Affect-inducing GIFs</title>
  <link rel="icon" type="image/x-icon" href="static/images/brain-512.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">Exploring Emotion Processing in the Human Brain through Positive and Negative Affect-inducing GIFs</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.researchgate.net/profile/Joshua-Rocha-2" target="_blank">Joshua A. Rocha</a><sup>1,5</sup>,</span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/vera-klütz-5a67a1283" target="_blank">Vera Klütz</a><sup>1,6</sup>,</span>
                  <span class="author-block">
                    <a href="https://www.zi-mannheim.de/en/research/people/person/15954.html" target="_blank">Gordon B. Feld</a><sup>1,2,3,4</sup>,</span>
                    <span class="author-block">
                      <a href="https://skjerns.de" target="_blank">Simon Kern</a><sup>1,2,3,4</sup></span>
                  </div>
                  <div class="is-size-6 publication-authors">
                    <span class="author-block">1 Department of Clinical Psychology, Central Institute of Mental Health, Medical Faculty Mannheim, University of Heidelberg, Mannheim, Germany
                      <br>2 Department of Psychiatry and Psychotherapy, Central Institute of Mental Health, Medical Faculty Mannheim, University of Heidelberg, Mannheim, Germany
                      <br>3 Department of Addiction Behaviour and Addiction Medicine, Central Institute of Mental Health, Medical Faculty Mannheim, University of Heidelberg, Mannheim, Germany
                      <br>4 Institute of Psychology, Faculty of Behavioural and Cultural Studies, University of Heidelberg, Heidelberg, Germany
                      <br>5 Heidelberg Academy of Sciences and Humanities, Heidelberg, Germany
                      <br>6 Institute of Cognitive Science, Osnabrück University, Osnabrück, Germany</span>
                      <span></span><a href="https://pug2024.de/index-en.html" target="_blank">Psychology & Brain 2024</a></span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>-->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="#poster-section" class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Poster</span>
                      </a>
                    </span>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div style="display: flex; justify-content: center; align-items: center;">
        <img src="static/videos/posi_0605.gif" autoplay loop height="200%" muted>
      </div>
      <h2 class="subtitle has-text-centered">
        <br>We aim to establish a machine learning algorithm capable of classifying positive and negative affect markers in brain signals of humans
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Poster Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Emotions are thought to influence activity in human brain areas that control decisions, direct attention, and motivate behavior in our surrounding world. Indeed, affective neuroscience has long since attempted to explore the underlying mechanisms of emotion processing that are interlocked with perception, cognition, motivation, and action in the brain. However, the organization of the anatomical and functional neural networks that overall form our emotion processing architecture are yet to be fully elucidated.<br>
            This study aimed to investigate this by measuring the positive and negative affect of a set of short video clips (GIFs), which have previously been validated and classified by thousands of participants into 27 distinct emotion categories, using Magnetoencephalography (MEG). Healthy participants are placed in the MEG scanner and subjected to a behavioral psychophysics task, in which a total of 144 positive or negative affect-inducing GIFs are shown in a randomized order and subsequently rated on a 5-point scale of the valence and arousal dimensions in each trial. As a next step (currently in progress), the obtained MEG signals will be analyzed using machine learning algorithms to extract neural markers of positive and negative affect processing.<br>
            These preliminary findings could potentially lead to an improved understanding of the neural networks involved in emotion processing and furthermore facilitate the development of novel translational approaches against affective disorders such as major depression and bipolar disorder, along with methods to detect affect processing in the absence of behavioral input in cases such as sleep or resting state memory consolidation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 is-centered has-text-centered">Experiment</h2>
       <div style="display: flex; justify-content: center; align-items: center;">
        <img src="static/images/JR_ERP_ExpDes_FC.png" autoplay loop height="100%" muted>
      </div>
      <h2 class="subtitle has-text-centered">
      <br><br>
      <br><br>
    </h2>
     
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay loop height="100%" muted>
        <!-- Your video here -->
        <source src="static/videos/JR_PuG_ExpMeth.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <b>An example of an experiment round</b>
      </h2>
        <h2 class="subtitle is-5 has-text-justified">
        In each trial, a red fixation cross first appears at the center of one of four quadrants on the screen to draw the attention of the participant to that part of the screen. 
        <br>One of the 144 GIFs then plays out and the participant pushes a button on a controller to indicate the moment they felt an emotion. 
        <br>The participant subsequently rates how positive or negative the emotion they felt was on a 5-point Valence scale, and how calm or excited the emotion they felt was on a 5-point Arousal scale. 
        <br>Finally, the participant performs a flanker task in which four arrows appear at four possible locations (north, south, east, or west) in a randomized order and they press buttons on the controller to indicate the direction the arrow is pointed towards.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->


<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="container is-max-desktop">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 is-centered has-text-centered">Data Analysis</h2>
    <br>
      
    <h3 class="title is-4 is-centered has-text-centered">Preprocessing</h3>
        <h2 class="subtitle is-5 has-text-justified">
          <br>The MEG data went through the following preprocessing steps:
           <ul>
            <li>- Highpass-, lowpass- and notch-filtering</li>
            <li>- Event extraction and epoching</li>
            <li>- Autorejection of bad epochs</li>
            <li>- ICA with rejection of ECG and EOG related signals</li>
           </ul>   
          <br>
          Additionally, as recent literature suggests only minimal preprocessing (Delorme, 2023b), a second version has been obtained with only highpassed epochs. Despite 
          a bigger confidence interval for minimal preprocessed data, no difference in classification accuracy could be found.
        </h2>
      <br>
      
      <h3 class="title is-4 is-centered has-text-centered">Classificaton steps</h3>
        <h2 class="subtitle is-5 has-text-justified">
          <br>First, we establish a classification pipeline. Therefore, we use and easy-to-test target, as decoding emotions will introduce more possible error sources. 
          Here, we decode in which of 4 corners the GIF was shown. As a machine learning classifier we use Logistic Regression and Random Forest, which yield similar results.
          The figure shows, at which timepoint it is possible to accurately classify the GIF's position.
          <br>
           Next, different features of the MEG signal were extracted to see how much they contribute to a correct classification. When extracting delta, theta, alpha, and beta frequency 
        bands, our classification results solely based on these bands yield a peak accuracy of 33%.
        <br>
        After the pipeline has been tested and established, the target will be changed to affect related items such as valence and arousal. Additionally, different features
          will be tested to see how much they contribute.
          </h2>
      
            <!-- Your image here -->
        <img src="static/images/data_analysis_flowchart.PNG" alt="steps the data goes through from raw to bein classified" height="60%"/>
      
    </div>
  </div>
  </div>
</section>
<!-- End youtube video -->


<!-- Video carousel -->
<section class="hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <br><br><br>
        <h2 class="title is-3 is-centered has-text-centered">Discussion</h2>
        <div class="content has-text-justified">
          <p>
            In this project, we aim to find a classifier that can successfully decode emotions. The preliminary findings could potentially lead to an improved understanding of the neural networks that are involved in emotion processing in humans. This would open up opportunities to facilitate the progress of novel translational approaches against increasingly relevant affective disorders such as major depression and bipolar disorder. Simultaneously, the development of methods that detect emotion affect processing in the absence of behavioral input in cases such as when one is asleep or when the memory consolidation process occurs during resting state.
          </p>
          <br>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->






<!-- Paper poster -->
<section class="hero is-small is-light" id="poster-section">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-centered has-text-centered">Poster</h2>

      <iframe  src="static/pdfs/JR_VK_PuG_Poster_ZI_hoch_final.pdf" width="100%" height="1400">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title is-centered has-text-centered">Literature</h2>
      <pre><code>
Brosch, T., Scherer, K., Grandjean, D., & Sander, D. (2013). The impact of emotion on perception, attention, memory, and decision-making. Swiss Medical Weekly, 143(1920), w13786. https://doi.org/10.4414/smw.2013.13786
Cowen, A. S., & Keltner, D. (2017). Self-report captures 27 distinct categories of emotion bridged by continuous gradients. Proceedings of the National Academy of Sciences, 114(38), E7900 – E7909. https://doi.org/10.1073/pnas.1702247114
Delorme, A. (2023b). EEG is better left alone. Scientific Reports, 13(1). https://doi.org/10.1038/s41598-023-27528-0     
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
