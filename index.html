<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="Exploring Emotion Processing in the Human Brain through Positive and Negative Affect-inducing GIFs"/>
  <meta property="og:description" content="Poster for PuG 2024"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Exploring Emotion Processing in the Human Brain through Positive and Negative Affect-inducing GIFs</title>
  <link rel="icon" type="image/x-icon" href="static/images/brain-512.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">Exploring Emotion Processing in the Human Brain through Positive and Negative Affect-inducing GIFs</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.researchgate.net/profile/Joshua-Rocha-2" target="_blank">Joshua A. Rocha</a><sup>1,5</sup>,</span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/vera-klütz-5a67a1283" target="_blank">Vera Klütz</a><sup>1,6</sup>,</span>
                  <span class="author-block">
                    <a href="https://www.zi-mannheim.de/en/research/people/person/15954.html" target="_blank">Gordon B. Feld</a><sup>1,2,3,4</sup>,</span>
                    <span class="author-block">
                      <a href="https://skjerns.de" target="_blank">Simon Kern</a><sup>1,2,3,4</sup></span>
                  </div>
                  <div class="is-size-6 publication-authors">
                    <span class="author-block">1 Department of Clinical Psychology, Central Institute of Mental Health, Medical Faculty Mannheim, University of Heidelberg, Mannheim, Germany
                      <br>2 Department of Psychiatry and Psychotherapy, Central Institute of Mental Health, Medical Faculty Mannheim, University of Heidelberg, Mannheim, Germany
                      <br>3 Department of Addiction Behaviour and Addiction Medicine, Central Institute of Mental Health, Medical Faculty Mannheim, University of Heidelberg, Mannheim, Germany
                      <br>4 Institute of Psychology, Faculty of Behavioural and Cultural Studies, University of Heidelberg, Heidelberg, Germany
                      <br>5 Heidelberg Academy of Sciences and Humanities, Heidelberg, Germany
                      <br>4 Institute of Cognitive Science, Osnabrück University, Osnabrück, Germany</span>
                      <span></span><a href="https://pug2024.de/index-en.html" target="_blank">Psychology & Brain 2024</a></span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>-->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="#poster-section" class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Poster</span>
                      </a>
                    </span>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div style="display: flex; justify-content: center; align-items: center;">
        <img src="static/videos/posi_0605.gif" autoplay loop height="150%" muted>
      </div>
      <h2 class="subtitle has-text-centered">
        <br>We aim to establish a machine learning algorithm capable of classifying positive and negative affect markers in brain signals of humans
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Poster Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Emotions are thought to influence activity in human brain areas that control decisions, direct attention, and motivate behavior in our surrounding world. Indeed, affective neuroscience has long since attempted to explore the underlying mechanisms of emotion processing that are interlocked with perception, cognition, motivation, and action in the brain. However, the organization of the anatomical and functional neural networks that overall form our emotion processing architecture are yet to be fully elucidated.<br>
            This study aimed to investigate this by measuring the positive and negative affect of a set of short video clips (GIFs), which have previously been validated and classified by thousands of participants into 27 distinct emotion categories, using Magnetoencephalography (MEG). Healthy participants are placed in the MEG scanner and subjected to a behavioral psychophysics task, in which a total of 144 positive or negative affect-inducing GIFs are shown in a randomized order and subsequently rated on a 5-point scale of the valence and arousal dimensions in each trial. As a next step (currently in progress), the obtained MEG signals will be analyzed using machine learning algorithms to extract neural markers of positive and negative affect processing.<br>
            These preliminary findings could potentially lead to an improved understanding of the neural networks involved in emotion processing and furthermore facilitate the development of novel translational approaches against affective disorders such as major depression and bipolar disorder, along with methods to detect affect processing in the absence of behavioral input in cases such as sleep or resting state memory consolidation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Methods</h2>
      <div style="display: flex; justify-content: center; align-items: center;">
        <img src="static/images/methods.PNG" autoplay loop height="100%" muted>
      </div>
      <h2 class="subtitle has-text-centered">
      <b>Methodological Approach:</b> The study's methodology entails using deep neural networks for behavioral response analysis. The Ecoset dataset is selected, containing over 1.5 million images across 565 object categories. The models utilized are AlexNet, VGG-16, and ResNet-50, and the performance of each is measured using the Ecoset test dataset to calculate inversion effects per category.
      <br><br>
      <b>Participant Selection:</b> 45 participants were chosen, including 28 females and 17 males aged 18-38 years. They engaged in a 10-Way-Classification-Task, with a focus on accuracy and latency of object recognition. Three participants were excluded due to abnormal response patterns and reaction times.
      <br><br>
      <b>Experimental Stimuli:</b> The experimental stimuli consisted of images from ten categories in the Ecoset test dataset, chosen to represent varying inversion effects. These categories ranged from objects like cogwheels to dolphins. The ten categories were chosen for their consistent inversion scores across all three models.
      <br><br>
      <b>Experimental Paradigm:</b> A 10-Way-Classification-Task was conducted to study inversion effects in humans and deep neural networks. Each participant/network was shown 100 images in both upright and inverted orientations and had to identify the category of each image. This was done under specific conditions, including a short presentation time of 100 ms, to ensure comparable feedforward processing to neural networks.
      <br><br>
    </h2>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->




<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Results</h2>
      <br>
      <h3 class="title is-4">Deep Neural Networks</h3>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/deep1.PNG" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          The research found that inversion significantly decreases recognition accuracy in all models (ResNet, AlexNet, and VGG-16), but the model architecture has no significant influence on the extent of the inversion effect. This result, consistent with previous studies, suggests that perceptual phenomena like the Thatcher effect or mirror confusion in deep neural networks may be due to previous experience with faces or objects, not the specific network architecture.
        </h2>
        </div>
        <div class="item">
        <!-- Your image here -->
        <img src="static/images/deep2.PNG" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          The degree of orientation significantly influences object recognition in deep neural networks, with all models (AlexNet, ResNet, VGG-16) showing best recognition performance at upright orientation and additional peaks at 90 and 180 degrees. These findings emphasize the importance of understanding preprocessing steps during training, such as data augmentation, which may have a greater impact on results than the choice of model architecture.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/deep3.PNG" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Inversion Effects from the selected object categories differ significantly. Stimuli from these categories were chosen for the behavioral study with finetuned deep nets and humans.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/deep4.PNG" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        A representational similarity analysis (RSA) was conducted on ResNet-50, AlexNet, and VGG-16 models, revealing no significant differences in activations in the penultimate layer of the neural networks based on the orientation of stimuli. Despite expectations, inverted and upright images are represented similarly in the networks' features, with late layers displaying more invariance to image transformations like rotation or translations.
      </h2>
    </div>
    </div>

    <br><br><br>
    <h3 class="title is-4">Humans</h3>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/human1.PNG" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          A paired-samples t-test revealed a significant decrease in recognition performance for inverted images compared to upright ones.
        </h2>
        </div>
        <div class="item">
        <!-- Your image here -->
        <img src="static/images/human2.PNG" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          A two-way repeated-measures ANOVA on recognition accuracy across object category and orientation revealed significant main effects for orientation and category, but not for their interaction, indicating no significant difference in inversion effects across groups. However, post-hoc analysis showed a significant decrease in recognition performance for inverted images in the 'weasel' category, decreasing recognition performance by about 5 percent. No significant differences in inversion effects were found across the other nine categories, and this may be attributed to efficient face recognition mechanisms being recruited during the recognition of animate objects and to the nature of the experimental design and the selected categories. It's suggested that object categories are processed holistically when exemplar individuation is needed, the category belongs to a homogeneous group, and the objects have a canonical orientation.
        </h2>
      </div>
    </div>

    <br><br><br>
    <h3 class="title is-4">Humans vs. Deep Neural Networks</h3>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/humandeep1.PNG" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          To investigate whether humans and neural networks exhibit similar behaviors, neural networks were modified to match a 10-way classification task, trained with the Ecoset pre-trained models, and tested on upright and inverted images.  Misclassifications
          in humans and neural networks mainly pointed to problems with classes that are
          conceptually similar. In my data, eggplants were misclassified as onions and vice versa.
        </h2>
        </div>
        <div class="item">
        <!-- Your image here -->
        <img src="static/images/humandeep2.PNG" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Results showed significantly high correlations in response patterns between humans and models, suggesting similarities in classification behaviors. When analyzing error distributions by excluding correct responses, correlations between humans and neural networks were found to be low to moderate, with ResNet and VGG-16 showing the highest similarity to human errors. However, divergent classification patterns between humans and deep neural networks (DNNs) emerged when the level of image distortion increased, with misclassifications often occurring among conceptually similar classes, suggesting that current neural network architectures may need further refinement to better emulate human behavior.
        </h2>
      </div>
    </div>


  </div>
</section>
<!-- End youtube video -->


<!-- Video carousel -->
<section class="hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <br><br><br>
        <h2 class="title is-3">Main Finding</h2>
        <div class="content has-text-justified">
          <p>
            This thesis investigated inversion effects (recognition accuracy for images presented upside down versus right side up) in both humans and deep neural networks (DNNs). The findings suggest that DNNs do exhibit significant inversion effects, which are also reflected across different object categories. However, the architecture of the neural network does not impact the magnitude of this effect. In humans, no significant differences were found across object categories, but the response patterns between humans and DNNs showed strong similarities. Future research could focus on identifying which object properties amplify these inversion effects in neural networks, and then confirm these properties in humans. This could help further understand the origins of the face inversion effect, which is the greater difficulty people have in recognizing faces presented upside down compared to other objects.
          </p>
          <br>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->






<!-- Paper poster -->
<section class="hero is-small is-light" id="poster-section">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/JR_VK_PuG_Poster_ZI_hoch_final.pdf" width="100%" height="1300">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title is-centered has-text-centered">Literature</h2>
      <pre><code>
Brosch, T., Scherer, K., Grandjean, D., & Sander, D. (2013). The impact of emotion on perception, attention, memory, and decision-making. Swiss Medical Weekly, 143(1920), w13786. https://doi.org/10.4414/smw.2013.13786
Cowen, A. S., & Keltner, D. (2017). Self-report captures 27 distinct categories of emotion bridged by continuous gradients. Proceedings of the National Academy of Sciences, 114(38), E7900 – E7909. https://doi.org/10.1073/pnas.1702247114
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
