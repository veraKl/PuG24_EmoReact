<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="Exploring Emotion Processing in the Human Brain through Positive and Negative Affect-inducing GIFs"/>
  <meta property="og:description" content="Poster for PuG 2024"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Exploring Emotion Processing in the Human Brain through Positive and Negative Affect-inducing GIFs</title>
  <link rel="icon" type="image/x-icon" href="static/images/brain-512.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">Exploring Emotion Processing in the Human Brain through Positive and Negative Affect-inducing GIFs</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.researchgate.net/profile/Joshua-Rocha-2" target="_blank">Joshua A. Rocha</a><sup>1,5</sup>,</span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/vera-klütz-5a67a1283" target="_blank">Vera Klütz</a><sup>1,6</sup>,</span>
                  <span class="author-block">
                    <a href="https://www.zi-mannheim.de/en/research/people/person/15954.html" target="_blank">Gordon B. Feld</a><sup>1,2,3,4</sup>,</span>
                    <span class="author-block">
                      <a href="https://skjerns.de" target="_blank">Simon Kern</a><sup>1,2,3,4</sup></span>
                  </div>
                  <div class="is-size-6 publication-authors">
                    <span class="author-block">1 Department of Clinical Psychology, Central Institute of Mental Health, Medical Faculty Mannheim, University of Heidelberg, Mannheim, Germany
                      <br>2 Department of Psychiatry and Psychotherapy, Central Institute of Mental Health, Medical Faculty Mannheim, University of Heidelberg, Mannheim, Germany
                      <br>3 Department of Addiction Behaviour and Addiction Medicine, Central Institute of Mental Health, Medical Faculty Mannheim, University of Heidelberg, Mannheim, Germany
                      <br>4 Institute of Psychology, Faculty of Behavioural and Cultural Studies, University of Heidelberg, Heidelberg, Germany
                      <br>5 Heidelberg Academy of Sciences and Humanities, Heidelberg, Germany
                      <br>4 Institute of Cognitive Science, Osnabrück University, Osnabrück, Germany</span>
                      <span></span><a href="https://pug2024.de/index-en.html" target="_blank">Psychology & Brain 2024</a></span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>-->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="#poster-section" class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Poster</span>
                      </a>
                    </span>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div style="display: flex; justify-content: center; align-items: center;">
        <img src="static/videos/posi_0605.gif" autoplay loop height="150%" muted>
      </div>
      <h2 class="subtitle has-text-centered">
        <br>We aim to establish a machine learning algorithm capable of classifying positive and negative affect markers in brain signals of humans
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Poster Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Emotions are thought to influence activity in human brain areas that control decisions, direct attention, and motivate behavior in our surrounding world. Indeed, affective neuroscience has long since attempted to explore the underlying mechanisms of emotion processing that are interlocked with perception, cognition, motivation, and action in the brain. However, the organization of the anatomical and functional neural networks that overall form our emotion processing architecture are yet to be fully elucidated.<br>
            This study aimed to investigate this by measuring the positive and negative affect of a set of short video clips (GIFs), which have previously been validated and classified by thousands of participants into 27 distinct emotion categories, using Magnetoencephalography (MEG). Healthy participants are placed in the MEG scanner and subjected to a behavioral psychophysics task, in which a total of 144 positive or negative affect-inducing GIFs are shown in a randomized order and subsequently rated on a 5-point scale of the valence and arousal dimensions in each trial. As a next step (currently in progress), the obtained MEG signals will be analyzed using machine learning algorithms to extract neural markers of positive and negative affect processing.<br>
            These preliminary findings could potentially lead to an improved understanding of the neural networks involved in emotion processing and furthermore facilitate the development of novel translational approaches against affective disorders such as major depression and bipolar disorder, along with methods to detect affect processing in the absence of behavioral input in cases such as sleep or resting state memory consolidation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Methods</h2>
      <div style="display: flex; justify-content: center; align-items: center;">
        <img src="static/images/methods.PNG" autoplay loop height="100%" muted>
      </div>
      <h2 class="subtitle has-text-centered">
      <b>Methodological Approach:</b> The study's methodology entails using deep neural networks for behavioral response analysis. The Ecoset dataset is selected, containing over 1.5 million images across 565 object categories. The models utilized are AlexNet, VGG-16, and ResNet-50, and the performance of each is measured using the Ecoset test dataset to calculate inversion effects per category.
      <br><br>
      <b>Participant Selection:</b> 45 participants were chosen, including 28 females and 17 males aged 18-38 years. They engaged in a 10-Way-Classification-Task, with a focus on accuracy and latency of object recognition. Three participants were excluded due to abnormal response patterns and reaction times.
      <br><br>
      <b>Experimental Stimuli:</b> The experimental stimuli consisted of images from ten categories in the Ecoset test dataset, chosen to represent varying inversion effects. These categories ranged from objects like cogwheels to dolphins. The ten categories were chosen for their consistent inversion scores across all three models.
      <br><br>
      <b>Experimental Paradigm:</b> A 10-Way-Classification-Task was conducted to study inversion effects in humans and deep neural networks. Each participant/network was shown 100 images in both upright and inverted orientations and had to identify the category of each image. This was done under specific conditions, including a short presentation time of 100 ms, to ensure comparable feedforward processing to neural networks.
      <br><br>
    </h2>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->




<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Results</h2>
      <br>
      <h3 class="title is-4">Deep Neural Networks</h3>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/deep1.PNG" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          The research found that inversion significantly decreases recognition accuracy in all models (ResNet, AlexNet, and VGG-16), but the model architecture has no significant influence on the extent of the inversion effect. This result, consistent with previous studies, suggests that perceptual phenomena like the Thatcher effect or mirror confusion in deep neural networks may be due to previous experience with faces or objects, not the specific network architecture.
        </h2>
        </div>
        <div class="item">
        <!-- Your image here -->
        <img src="static/images/deep2.PNG" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          The degree of orientation significantly influences object recognition in deep neural networks, with all models (AlexNet, ResNet, VGG-16) showing best recognition performance at upright orientation and additional peaks at 90 and 180 degrees. These findings emphasize the importance of understanding preprocessing steps during training, such as data augmentation, which may have a greater impact on results than the choice of model architecture.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/deep3.PNG" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Inversion Effects from the selected object categories differ significantly. Stimuli from these categories were chosen for the behavioral study with finetuned deep nets and humans.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/deep4.PNG" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        A representational similarity analysis (RSA) was conducted on ResNet-50, AlexNet, and VGG-16 models, revealing no significant differences in activations in the penultimate layer of the neural networks based on the orientation of stimuli. Despite expectations, inverted and upright images are represented similarly in the networks' features, with late layers displaying more invariance to image transformations like rotation or translations.
      </h2>
    </div>
    </div>

    <br><br><br>
    <h3 class="title is-4">Humans</h3>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/human1.PNG" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          A paired-samples t-test revealed a significant decrease in recognition performance for inverted images compared to upright ones.
        </h2>
        </div>
        <div class="item">
        <!-- Your image here -->
        <img src="static/images/human2.PNG" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          A two-way repeated-measures ANOVA on recognition accuracy across object category and orientation revealed significant main effects for orientation and category, but not for their interaction, indicating no significant difference in inversion effects across groups. However, post-hoc analysis showed a significant decrease in recognition performance for inverted images in the 'weasel' category, decreasing recognition performance by about 5 percent. No significant differences in inversion effects were found across the other nine categories, and this may be attributed to efficient face recognition mechanisms being recruited during the recognition of animate objects and to the nature of the experimental design and the selected categories. It's suggested that object categories are processed holistically when exemplar individuation is needed, the category belongs to a homogeneous group, and the objects have a canonical orientation.
        </h2>
      </div>
    </div>

    <br><br><br>
    <h3 class="title is-4">Humans vs. Deep Neural Networks</h3>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/humandeep1.PNG" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          To investigate whether humans and neural networks exhibit similar behaviors, neural networks were modified to match a 10-way classification task, trained with the Ecoset pre-trained models, and tested on upright and inverted images.  Misclassifications
          in humans and neural networks mainly pointed to problems with classes that are
          conceptually similar. In my data, eggplants were misclassified as onions and vice versa.
        </h2>
        </div>
        <div class="item">
        <!-- Your image here -->
        <img src="static/images/humandeep2.PNG" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Results showed significantly high correlations in response patterns between humans and models, suggesting similarities in classification behaviors. When analyzing error distributions by excluding correct responses, correlations between humans and neural networks were found to be low to moderate, with ResNet and VGG-16 showing the highest similarity to human errors. However, divergent classification patterns between humans and deep neural networks (DNNs) emerged when the level of image distortion increased, with misclassifications often occurring among conceptually similar classes, suggesting that current neural network architectures may need further refinement to better emulate human behavior.
        </h2>
      </div>
    </div>


  </div>
</section>
<!-- End youtube video -->


<!-- Video carousel -->
<section class="hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <br><br><br>
        <h2 class="title is-3">Main Finding</h2>
        <div class="content has-text-justified">
          <p>
            This thesis investigated inversion effects (recognition accuracy for images presented upside down versus right side up) in both humans and deep neural networks (DNNs). The findings suggest that DNNs do exhibit significant inversion effects, which are also reflected across different object categories. However, the architecture of the neural network does not impact the magnitude of this effect. In humans, no significant differences were found across object categories, but the response patterns between humans and DNNs showed strong similarities. Future research could focus on identifying which object properties amplify these inversion effects in neural networks, and then confirm these properties in humans. This could help further understand the origins of the face inversion effect, which is the greater difficulty people have in recognizing faces presented upside down compared to other objects.
          </p>
          <br>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->






<!-- Paper poster -->
<section class="hero is-small is-light" id="poster-section">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/JR_VK_PuG_Poster_ZI_hoch_final.pdf" width="100%" height="1300">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">References</h2>
      <pre><code>
Andina, D., Vega-Corona, A., Seijas, J. I., & Torres-Garcìa, J. (2007). Neural networks historical review. In Computational Intelligence (pp. 39-65). Springer, Boston, MA.
Ashworth III, A. R., Vuong, Q. C., Rossion, B., & Tarr, M. J. (2008). Recognizing rotated faces and Greebles: What properties drive the face inversion effect?. Visual Cognition,16(6), 754-784.
Baek, S., Song, M., Jang, J., Kim, G., & Paik, S. B. (2021). Face detection in untrained deep neural networks. Nature communications, 12(1), 1-15.
Baker, N., Lu, H., Erlikhman, G., & Kellman, P. J. (2018). Deep convolutional networks do not classify based on global object shape. PLoS computational biology, 14(12), e1006613.
Bartlett, J. C., & Searcy, J. (1993). Inversion and configuration of faces. Cognitive psychology, 25(3), 281-316.
Bejnordi, B. E., Veta, M., Van Diest, P. J., Van Ginneken, B., Karssemeijer, N., Litjens, G., ... & CAMELYON16 Consortium. (2017). Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer. Jama, 318(22), 2199-2210.
Belanova, E., Davis, J. P., & Thompson, T. (2021). The part-whole effect in super-recognisers and typical-range-ability controls. Vision Research, 187, 75-84.
Berg, A. (2018). LeCun, Bengio and Hinton [Photograph]. Communications of the ACM. https://cacm.acm.org/magazines/2019/6/236990-neural-net-worth/abstract
Box, G. E. (1979). Robustness in the strategy of scientific model building. In Robustness in statistics (pp. 201-236). Academic Press.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33, 1877-1901.
Bruyer, R. (2011). Configural face processing: A meta-analytic survey. Perception, 40(12), 1478-1490.
Burton, A. M., White, D., & McNeill, A. (2010). The Glasgow face matching test. Behavior research methods, 42(1), 286-291.
Busigny, T., & Rossion, B. (2010). Acquired prosopagnosia abolishes the face inversion effect. Cortex, 46(8), 965-981.
Campbell, A., & Tanaka, J. W. (2018). Inversion impairs expert budgerigar identity recognition: a face-like effect for a nonface object of expertise. Perception, 47(6), 647-659. Inversion Effects in Humans and Deep Neural Networks 47
Chicco, D. (2021). Siamese neural networks: An overview. Artificial Neural Networks, 73-94.
Cichy, R. M., & Kaiser, D. (2019). Deep neural networks as scientific models. Trends in cognitive sciences, 23(4), 305-317.
Cohen, J. (1988). Statistical power analysis for the behavioral sciences, 2nd ed. Hillsdale, NJ: Erlbaum.
Collishaw, S. M., & Hole, G. J. (2000). Featural and configurational processes in the recognition of faces of different familiarity. Perception, 29(8), 893-909.
Crookes, K., & McKone, E. (2009). Early maturity of face recognition: No childhood development of holistic processing, novel face encoding, or face-space. Cognition, 111(2), 219-247.
De Gelder, B., Bachoud-Lévi, A. C., & Degos, J. D. (1998). Inversion superiority in visual agnosia may be common to a variety of orientation polarised objects besides faces. Vision research, 38(18), 2855-2861.
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition (pp. 248–255).
Diamond, R., & Carey, S. (1986). Why faces are and are not special: an effect of expertise. Journal of experimental psychology: general, 115(2), 107.
Dodge, S., & Karam, L. (2017). A study and comparison of human and deep learning recognition performance under visual distortions. In 2017 26th international conference on computer communication and networks (ICCCN) (pp. 1-7). IEEE.
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... &
Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.
Elmahmudi, A., & Ugail, H. (2019). Deep face recognition using imperfect facial data. Future Generation Computer Systems, 99, 213-225.
Engstrom, L., Tran, B., Tsipras, D., Schmidt, L., & Madry, A. (2018). A rotation and a translation suffice: Fooling cnns with simple transformations.
Farah, M. J., Tanaka, J. W., & Drain, H. M. (1995). What causes the face inversion effect?. Journal of Experimental Psychology: Human perception and performance, 21(3), 628.
Farah, M. J., Wilson, K. D., Drain, H. M., & Tanaka, J. R. (1995). The inverted face inversion effect in prosopagnosia: Evidence for mandatory, face-specific perceptual mechanisms. Vision research, 35(14), 2089-2093. Inversion Effects in Humans and Deep Neural Networks 48
Faul, F., Erdfelder, E., Buchner, A., & Lang, A.-G. (2009). Statistical power analyses using G*Power 3.1: Tests for correlation and regression analyses. Behavior Research Methods, 41, 1149-1160.
Fawzi, A., & Frossard, P. (2015). Manitest: Are classifiers really invariant?. arXiv preprint arXiv:1507.06535.
Gale, W., Oakden-Rayner, L., Carneiro, G., Bradley, A. P., & Palmer, L. J. (2017). Detecting hip fractures with radiologist-level performance using deep neural networks. arXiv preprint arXiv:1711.06504.
Garrido, L., Susilo, T., Rezlescu, C., & Duchaine, B. (2019). Probing the Origins of the Face Inversion Effect With an Extraordinary Participant. Perception, 48(2).
Gauthier, I., Skudlarski, P., Gore, J. C., & Anderson, A. W. (2000). Expertise for cars and birds recruits brain areas involved in face recognition. Nature neuroscience, 3(2), 191-197.
Geirhos, R., Janssen, D. H., Schütt, H. H., Rauber, J., Bethge, M., & Wichmann, F. A. (2017). Comparing deep neural networks against humans: object recognition when the signal gets weaker. arXiv preprint arXiv:1706.06969.
Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F. A., & Brendel, W. (2018). ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. arXiv preprint arXiv:1811.12231.
Geirhos, R., Temme, C. R., Rauber, J., Schütt, H. H., Bethge, M., & Wichmann, F. A. (2018). Generalisation in humans and deep neural networks. Advances in neural information processing systems, 31.
Ghodrati, M., Farzmahdi, A., Rajaei, K., Ebrahimpour, R., & Khaligh-Razavi, S. M. (2014).Feedforward object-vision models only tolerate small image variations compared tohuman. Frontiers in computational neuroscience, 8, 74.
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio,Y. (2014). Generative adversarial nets. Advances in neural information processing systems, 27.
Goren, C. C., Sarty, M., & Wu, P. Y. (1975). Visual following and pattern discrimination of face-like stimuli by newborn infants. Pediatrics, 56(4), 544-549.
Hamm, J. P., & McMullen, P. A. (1998). Effects of orientation on the identification of rotated objects depend on the level of identity. Journal of Experimental Psychology: Human Perception and Performance, 24(2), 413.
Haxby, J. V., Ungerleider, L. G., Clark, V. P., Schouten, J. L., Hoffman, E. A., & Martin, A. (1999). The effect of face inversion on activity in human neural systems for face and object perception. Neuron, 22(1), 189-199. Inversion Effects in Humans and Deep Neural Networks 49
He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778). Hermann, K., Chen, T., & Kornblith, S. (2020). The origins and prevalence of texture bias in convolutional neural networks. Advances in Neural Information Processing Systems, 33, 19000-19015
Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700-4708).
Isik, L., Meyers, E. M., Leibo, J. Z., & Poggio, T. (2014). The dynamics of invariant object recognition in the human visual system. Journal of neurophysiology, 111(1), 91-102.
Jacob, G., Pramod, R. T., Katti, H., & Arun, S. P. (2021). Qualitative similarities and differences in visual object representations between brains and deep networks. Nature communications, 12(1), 1-14.
Johnson, M. H., Dziurawiec, S., Ellis, H., & Morton, J. (1991). Newborns' preferential tracking of face-like stimuli and its subsequent decline. Cognition, 40(1-2), 1-19.
Johnson, M. H. (2005). Subcortical face processing. Nature Reviews Neuroscience, 6(10), 766-774.
Johnson, M. H. (2011). Face perception: A developmental perspective. The Oxford handbook of face perception, 3-14.
Jolicoeur, P. (1985). The time to name disoriented natural objects. Memory & cognition, 13(4), 289-303.
Jozwik, K. M., Kriegeskorte, N., Storrs, K. R., & Mur, M. (2017). Deep convolutional neural networks outperform feature-based but not categorical models in explaining object similarity judgments. Frontiers in psychology, 8, 1726.
Kanwisher, N., McDermott, J., & Chun, M. M. (1997). The fusiform face area: a module in human extrastriate cortex specialized for face perception. Journal of neuroscience, 17(11), 4302-4311.
Kanwisher, N., Tong, F., & Nakayama, K. (1998). The effect of face inversion on the human fusiform face area. Cognition, 68(1), B1-B11.
Karras, T., Aittala, M., Laine, S., Härkönen, E., Hellsten, J., Lehtinen, J., & Aila, T. (2021). Alias-free generative adversarial networks. Advances in Neural Information Processing Systems, 34, 852-863.Inversion Effects in Humans and Deep Neural Networks 50
Khaligh-Razavi, S. M., & Kriegeskorte, N. (2014). Deep supervised, but not unsupervised, models may explain IT cortical representation. PLoS computational biology, 10(11), e1003915.
Kheradpisheh, S. R., Ghodrati, M., Ganjtabesh, M., & Masquelier, T. (2016). Deep networks can resemble human feed-forward vision in invariant object recognition. Scientific reports, 6(1), 1-24.
Kietzmann, T. C., McClure, P., & Kriegeskorte, N. (2018). Deep neural networks in computational neuroscience. BioRxiv, 133504.
Konkle, T., Brady, T. F., Alvarez, G. A., & Oliva, A. (2010). Conceptual distinctiveness supports detailed visual long-term memory for real-world objects. Journal of experimental Psychology: general, 139(3), 558.
Kriegeskorte, N., Mur, M., & Bandettini, P. A. (2008). Representational similarity analysis-connecting the branches of systems neuroscience. Frontiers in systems neuroscience, 4.
Kriegeskorte, N. (2015). Deep neural networks: a new framework for modelling biological vision and brain information processing. biorxiv, 029876.
Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25.
Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2017). Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6), 84-90.
Kubilius, J., Bracci, S., & Op de Beeck, H. P. (2016). Deep neural networks as a computational model for human shape sensitivity. PLoS computational biology, 12(4), e1004896.
Kubilius, J., Schrimpf, M., Nayebi, A., Bear, D., Yamins, D. L., & DiCarlo, J. J. (2018). Cornet: Modeling the neural mechanisms of core object recognition. BioRxiv, 408385.
Lake, B. M., Zaremba, W., Fergus, R., & Gureckis, T. M. (2015, July). Deep Neural Networks Predict Category Typicality Ratings for Images. In CogSci.
LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. nature, 521(7553), 436-444.
Leder, H., & Carbon, C. C. (2006). Face‐specific configural processing of relational information. British Journal of Psychology, 97(1), 19-29.
Lee, K., Anzures, G., Quinn, P. C., Pascalis, O., & Slater, A. (2011). Development of face processing expertise. The Oxford handbook of face perception, 753-778.
Lewis, M. B., & Edmonds, A. J. (2003). Face detection: Mapping human performance. Perception, 32(8), 903-920.
Lewis, M. B. (2001). The Lady's not for turning: Rotation of the Thatcher illusion. Perception, 30(6), 769-774. Inversion Effects in Humans and Deep Neural Networks 51
Lindsay, G. W. (2021). Convolutional neural networks as a model of the visual system: Past, present, and future. Journal of cognitive neuroscience, 33(10), 2017-2031.
Liu, J., Li, J., Feng, L., Li, L., Tian, J., & Lee, K. (2014). Seeing Jesus in toast: neural and behavioral correlates of face pareidolia. Cortex, 53, 60-77.
Logothetis, N. K., & Sheinberg, D. L. (1996). Visual object recognition. Annual review of neuroscience, 19(1), 577-621.
Macchi Cassia, C., Turati, C., & Simion, F. (2004). Can a nonspecific bias toward top-heavy patterns explain newborns' face preference?. Psychological Science, 15(6), 379-383.
Macchi Cassia, V., Simion, F., & Umiltaà, C. (2001). Face preference at birth: the role of an orienting mechanism. Developmental Science, 4(1), 101-108.
Maheshwari, H. (2022). Performance degradation of ImageNet trained models by simple image transformations. arXiv preprint arXiv:2207.08079.
Maurer, D., Le Grand, R., & Mondloch, C. J. (2002). The many faces of configural processing. Trends in cognitive sciences, 6(6), 255-260.
Mehrer, J., Spoerer, C. J., Jones, E. C., Kriegeskorte, N., & Kietzmann, T. C. (2021). An ecologically motivated image dataset for deep learning yields better models of human vision. Proceedings of the National Academy of Sciences, 118(8), e2011417118.
Miller, G. A. (1995). WordNet: a lexical database for English. Communications of the ACM, 38(11), 39-41.
Murray, J. E., Yong, E., & Rhodes, G. (2000). Revisiting the perception of upside-down faces. Psychological Science, 11(6), 492-496.
Muttenthaler, L., & Hebart, M. N. (2021). THINGSvision: a Python toolbox for streamlining the extraction of activations from deep neural networks. Frontiers in neuroinformatics, 45.
Oliva, A., & Torralba, A. (2007). The role of context in object recognition. Trends in cognitive sciences, 11(12), 520-527.
Palmer, S. E. (1999). Vision science: Photons to phenomenology. MIT press.
Parkhi, O. M., Vedaldi, A., & Zisserman, A. (2015). Deep face recognition.
Pascalis, O., de Schonen, S., Morton, J., Deruelle, C., & Fabre-Grenet, M. (1995). Mother's face recognition by neonates: A replication and an extension. Infant Behavior and Development, 18(1), 79-85.
Perry, C. J., & Fallah, M. (2014). Feature integration and object representations along the dorsal stream visual hierarchy. Frontiers in computational neuroscience, 8, 84.
Peterson, J. C., Abbott, J. T., & Griffiths, T. L. (2017). Adapting Deep Network Features to Capture Psychological Representations: An Abridged Report. In IJCAI (pp. 4934-4938). Inversion Effects in Humans and Deep Neural Networks 52
Piepers, D. W., & Robbins, R. A. (2012). A review and clarification of the terms “holistic,”“configural,” and “relational” in the face perception literature. Frontiers in psychology, 3, 559.
Praß, M., Grimsen, C., König, M., & Fahle, M. (2013). Ultra rapid object categorization: effects of level, animacy and context. PLoS One, 8(6), e68051.
Rajalingham, R., Issa, E. B., Bashivan, P., Kar, K., Schmidt, K., & DiCarlo, J. J. (2018). Large-scale, high-resolution comparison of the core visual object recognition behavior of humans, monkeys, and state-of-the-art deep artificial neural networks. Journal of Neuroscience, 38(33), 7255-7269
Rakover, S. S. (2013). Explaining the face-inversion effect: the face–scheme incompatibility (FSI) model. Psychonomic bulletin & review, 20(4), 665-692.
Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., & Chen, M. (2022). Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125.
Reed, C. L., Stone, V. E., Bozova, S., & Tanaka, J. (2003). The body-inversion effect. Psychological science, 14(4), 302-308.
Rezlescu, C., Chapman, A., Susilo, T., & Caramazza, A. (2016). Large inversion effects are not specific to faces and do not vary with object expertise.
Rezlescu, C., Danaila, I., Miron, A., & Amariei, C. (2020). More time for science: Using Testable to create and share behavioral experiments faster, recruit better participants, and engage students in hands-on research. Progress in Brain Research, 253, 243-262.
Rhodes, G., Brake, S., & Atkinson, A. P. (1993). What's lost in inverted faces?. Cognition, 47(1), 25-57.
Rhodes, G., Brake, S., Taylor, K., & Tan, S. (1989). Expertise and configural coding in face recognition. British journal of psychology, 80(3), 313-331.
Rhodes, G. (1988). Looking at faces: First-order and second-order features as determinants of facial appearance. Perception, 17(1), 43-63.
Roads, B. D., & Love, B. C. (2021). Enriching imagenet with human similarity judgments and psychological embeddings. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 3547-3557).
Robbins, R., & McKone, E. (2007). No face-like processing for objects-of-expertise in three behavioural tasks. Cognition, 103(1), 34-79.
Roe, A. W., Chelazzi, L., Connor, C. E., Conway, B. R., Fujita, I., Gallant, J. L., ... &
Vanduffel, W. (2012). Toward a unified theory of visual area V4. Neuron, 74(1), 12-29.
Rosch, E. (1978). Principles of categorization.
Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating errors. nature, 323(6088), 533-536. Inversion Effects in Humans and Deep Neural Networks 53
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., ... & Fei-Fei, L. (2015). Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3), 211-252.
Schrimpf, M., Kubilius, J., Hong, H., Majaj, N. J., Rajalingham, R., Issa, E. B., ... & DiCarlo, J.J. (2020). Brain-score: Which artificial neural network for object recognition is most brain-like?. BioRxiv, 407007.
Schrimpf, M., Kubilius, J., Lee, M. J., Murty, N. A. R., Ajemian, R., & DiCarlo, J. J. (2020). Integrative benchmarking to advance neurally mechanistic models of human intelligence. Neuron, 108(3), 413-423.
Searcy, J. H., & Bartlett, J. C. (1996). Inversion and processing of component and spatial–relational information in faces. Journal of Experimental Psychology: Human Perception and Performance, 22(4), 904.
Shapiro, P. N., & Penrod, S. (1986). Meta-analysis of facial identification studies. Psychological bulletin, 100(2), 139.
Sharma, S., Sharma, S., & Athaiya, A. (2017). Activation functions in neural networks. towards data science, 6(12), 310-316.
Simion, F., Di Giorgio, E., Leo, I., & Bardi, L. (2011). The processing of social stimuli in early infancy: from faces to biological motion perception. Progress in brain research, 189, 173-193.
Simion, F., Valenza, E., Umilta, C., & Barba, B. D. (1998). Preferential orienting to faces in newborns: A temporal–nasal asymmetry. Journal of Experimental Psychology: Human perception and performance, 24(5), 1399.
Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.
Slater, A., Von der Schulenburg, C., Brown, E., Badenoch, M., Butterworth, G., Parsons, S., & Samuels, C. (1998). Newborn infants prefer attractive faces. Infant Behavior and Development, 21(2), 345-354.
Storrs, K. R., & Kriegeskorte, N. (2019). Deep learning for cognitive neuroscience. arXiv preprint arXiv:1903.01458.
Taigman, Y., Yang, M., Ranzato, M. A., & Wolf, L. (2014). Deepface: Closing the gap to human-level performance in face verification. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1701-1708).
Tanaka, J. W., & Gordon, I. (2011). Features, configuration, and holistic face processing. The Oxford handbook of face perception, 177-194.
Tanaka, J. W., & Sengco, J. A. (1997). Features and their configuration in face recognition. Memory & cognition, 25(5), 583-592. Inversion Effects in Humans and Deep Neural Networks 54
Tarr, M. J., & Gauthier, I. (2000). FFA: a flexible fusiform area for subordinate-level visual processing automatized by expertise. Nature neuroscience, 3(8), 764-769.
Thorpe, S., Fize, D., & Marlot, C. (1996). Speed of processing in the human visual system. nature, 381(6582), 520-522.
Tian, F., Xie, H., Song, Y., Hu, S., & Liu, J. (2022). The Face Inversion Effect in Deep Convolutional Neural Networks. Frontiers in Computational Neuroscience, 16.
Turati, C., Sangrigoli, S., Ruely, J., & de Schonen, S. (2004). Evidence of the face inversion effect in 4‐month‐old infants. Infancy, 6(2), 275-297.
Valentine, T. (1988). Upside‐down faces: A review of the effect of inversion upon face recognition. British journal of psychology, 79(4), 471-491.
Van Dyck, L. E., Kwitt, R., Denzler, S. J., & Gruber, W. R. (2021). Comparing Object Recognition in Humans and Deep Convolutional Neural Networks—An Eye Tracking Study. Frontiers in Neuroscience, 15, 750639.
VanRullen, R. (2017). Perception science in the age of deep neural networks. Frontiers in psychology, 8, 142.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.
Virtanen, P., Gommers, R., Oliphant, T. E., Haberland, M., Reddy, T., Cournapeau, D., ... & Van Mulbregt, P. (2020). SciPy 1.0: fundamental algorithms for scientific computing in Python. Nature methods, 17(3), 261-272.
Yamashita, R., Nishio, M., Do, R. K. G., & Togashi, K. (2018). Convolutional neural networks: an overview and application in radiology. Insights into imaging, 9(4), 611-629.
Yamins, D. L., & DiCarlo, J. J. (2016). Using goal-driven deep learning models to understand sensory cortex. Nature neuroscience, 19(3), 356-365.
Yan, X., Young, A. W., & Andrews, T. J. (2017). The automaticity of face perception is influenced by familiarity. Attention, Perception, & Psychophysics, 79(7), 2202-2211.
Yin, R. K. (1969). Looking at upside-down faces. Journal of experimental psychology, 81(1), 141.
Young, A. W., & Burton, A. M. (2018). Are we face experts?. Trends in cognitive sciences, 22(2), 100-110.
Young, A. W., Hellawell, D., & Hay, D. C. (1987). Configurational information in face perception. Perception, 42(11), 1166-1178.
Young, A. W., Hellawell, D., & Hay, D. C. (2013). Configurational information in face perception. Perception, 42(11), 1166-1178.
Yovel, G., & Kanwisher, N. (2005). The neural basis of the behavioral face-inversion effect. Current biology, 15(24), 2256-2262. Inversion Effects in Humans and Deep Neural Networks 55
Yovel, G., Pelc, T., & Lubetzky, I. (2010). It's all in your head: why is the body inversion effect abolished for headless bodies?. Journal of Experimental Psychology: Human Perception and Performance, 36(3), 759.
Zeiler, M. D., & Fergus, R. (2014, September). Visualizing and understanding convolutional networks. In European conference on computer vision (pp. 818-833). Springer, Cham
Zhang, M., Tseng, C., & Kreiman, G. (2020). Putting visual object recognition in context. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 12985-12994)
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
